(Type): multiplechoice
(Category): 6
(Random answers): 0

# Answers
1. (Correct): B
(Points): 1



# Questions 




1. (Question): Given a normally distributed data set having missing values which are spread along 1 standard deviation from the median. What percentage of data is surely unaffected?
>  (A): 68

> (B): 32

> (C): cannot be determined

> (D): less than 1

(Correct): B (Points): 1




(Type): multiplechoice
(Category): 6
(Random answers): 0


2. (Question): For the given values of x
[1,1,1,0,1,0,1,2]
 What is the entropy of x? 

>(A): 5/8 log(5/8) + 1/4 log(1/4) + 1/8 log(1/8)

> (B): -(3/8 log(5/8) + 1/4 log(1/4) + 5/8 log(1/8))

> (C): -(5/8 log(5/8) + 1/4 log(1/4) + 1/8 log(1/8))

> (D): 3/8 log(5/8) + 1/4 log(1/4) + 5/8 log(1/8)
(Correct): C
(Points): 1


(Type): multiplechoice
(Category): 6
(Random answers): 0


3. (Question): Variance can be interpreted using which of the following inequality ?
>(A): Testory
>
>(B): Stautaory
>
>(C): Chebyshev
>
>(D): All of the Mentioned


(Correct): C
(Points): 1


(Type): matching
(Matching style): 0
(Points style): 0
(Random matches): 0

4 (Question): For a binary classification problem after training on training dataset the following values are obtained for confusion matrix:


[table][tr][td]n=165[/td][td]Predicted No[/td][td]Predicted Yes[/td][/tr][tr][td]Actual No[/td][td]50[/td][td]10[/td][/tr][tr][td]Actual Yes[/td][td]5[/td][td]100[/td][/tr][/table]



Match the options below:
(A Clue): Classification Rate/Accuracy
(A Match): 0.90
(B Clue): Recall
(B Match): 0.95
(C Clue): Precision
(C Match): 0.91
(D Clue): F-measure
(D Match): 0.92
(E Clue): Prevalence of 'Yes'
(E Match): 0.64
(Category): 6
(Grade style): 0
(Points): 1


(Type): multiplechoice
(Category): 6
(Random answers): 1

6. (Question): Which of the following is not an example of a discrete probability distribution?
> (A): The number of bathrooms in a house
> 
> (B): Whether or not a home has a swimming pool in it.
>
> (C): Number of Bedrooms with balcony
> (D): Price of the house
> (Correct): D
> (Points): 1


(Type): multiplechoice
(Category): 6
(Random answers): 1

7. (Question): What is true for Poisson distribution?

>(A): The probability of success is always greater than 7
>
>(B): The mean and variance of the distribution are same (equal)
>
>(C): It always contains a contingency table
>
>(D): The number of trials are bounded

(Correct): B
(Points): 1


(Type): multiplechoice multiple answers
(Category): 6
(Grade style): 0
(Random answers): 1

8. (Question): Which of the following are correct assumptions in logistic regression?

>(A): The outcome is a binary or dichotomous variable like yes vs no, positive vs negative, 1 vs 0.
>
> (B): There is a non linear relationship between the logit of the outcome and each predictor variables. 
>
> (C): There is no high intercorrelations (i.e. multicollinearity) among the predictors.
>
> (D): There is no influential values (extreme values or outliers) in the continuous predictors
(Correct): A,C,D
(Points): 1


(Type): multiplechoice
(Category): 6
(Random answers): 1

9. (Question): What is generalization error in terms of the SVM?
>(A): Distance of hyperplane from the support vectors.
>   
> (B): The threshold amount of error
>
> (C): Accuracy of SVM on unseen data.
>
> (D): Accuracy of SVM in cross validation



(Correct): C
(Points): 1


// redo

10. (Type): multiplechoice multiple answers
(Category): 6
(Grade style): 0
(Random answers): 1
(Question): What is/are true about kernel in SVM?
(A): Kernel function map low dimensional data to high dimensional space
(B): It’s a similarity function
(C): Kernels can be used in algorithms other than SVM.
(D): Kernels cannot be used in algorithms other than SVM.
(Correct): A,B,C
(Points): 1


(Type): multiplechoice
(Category): 6
(Random answers): 1

11. (Question): Given the least square solution scenario, let y be the ground truth, y* be the solution and ∅ be the feature space. Then (y - y*) lies in the .......

(A): C (∅).
(B): Span ( C ( ∅ ) ).
(C): ∅ space.
(D): None of the above.
(Correct): D
(Points): 1


(Type): multiplechoice multiple answers
(Category): 6
(Grade style): 0
(Random answers): 1

12. (Question): Which of the following addresses over-fitting?
(A): Dropout
(B): Maximum a posteriori estimation
(C): Regularization
(D): Support Vector Regression


(Correct): A, C, D
(Points): 1


(Type): multiplechoice
(Category): 6
(Random answers): 0

13. (Question): Which of the following can learn Identity solution?
(A): Alexnet
(B): Inception
(C): Resnet
(D): ZFnet
(Correct): C
(Points): 1

// redo
(Type): multiplechoice multiple answers
(Category): 6
(Grade style): 0
(Random answers): 0
(Question): Which of the following is/are true for soft-margin SVM?
(A): non-zero ∈-band
(B): zero ∈-band
(C): can prevent over-fitting
(D): cannot prevent over-fitting
(Correct): A,C
(Points): 1


(Type): multiplechoice
(Category): 6
(Random answers): 0


(Question): Each tree of Random Forest has ....
(A): high variance, low bias
(B): low variance, high bias
(C): low variance, low bias
(D): high variance, high bias



(Correct): A
(Points): 1

//redo
(Type): matching
(Matching style): 0
(Points style): 0
(Random matches): 2

(Question): Match the options below:
(A Clue): Convolution
(A Match): Invariant Knowledge
(B Clue): Pooling
(B Match): Lose precise position
(C Clue): Fully Connected
(C Match): Combine local features
(D Clue): Dropout
(D Match): Cease dependence among units
(Category): 6
(Grade style): 2
(Points): 1


(Type): multiplechoice
(Category): 6
(Grade style): 0
(Random answers): 0
(Question): What are the true statements about null hypothesis of two sample student's t test?
(A): Mean of two samples are same.
(B): Mean of two samples are not same.
(C): Variance of two samples are same.
(D): Variance of two samples are not same.
(Correct): A
(Points): 1


(Type): multiplechoice
(Category): 6
(Grade style): 0
(Random answers): 0
(Question): Which of the following statements are true?
(A): L2 regularized regression can be used as a feature selection technique
(B): L1 regularized regression can be used as a feature selection technique
(C): Both L1 and L2 regularized regressions can be used as a feature selection technique
(D): None of the L1 and L2 regularized regressions can be used as a feature selection technique
(Correct): B
(Points): 1


(Type): truefalse
(Category): 6
(Random answers): 0
(Question): Condition node of a decision tree can be combination of 2 or more features.
(A): True
(B): False
(Correct): B
(Points): 1


(Type): multiplechoice
(Category): 6
(Random answers): 1
(Question): Which of the following layer doesn't contain any parameter to learn/optimize?
(A): Maxpool
(B): Convolution
(C): Dropout
(D): Batch-Norm
(Correct): A
(Points): 1


(Type): multiplechoice multiple answers
(Category): 6
(Grade style): 0
(Random answers): 0
(Question): Which of the following algorithm's standard implementation uses any distance/similarity measures such as euclidean, cosine etc?
(A): Support Vector Machine
(B): Random forest regression
(C): K Means
(D): K Nearest Neighbour
(Correct): C,D
(Points): 1


// redo
(Type): matching
(Matching style): 0
(Points style): 0
(Random matches): 2
(Question): Match the pairs
(A Clue): One way ANOVA
(A Match): Categorical and numeric var
(B Clue): Chi Square
(B Match): 2 categorical var
(C Clue): 2 Sample t test
(C Match): 2 numeric var
(Category): 6
(Grade style): 0
(Points): 1


// redo

(Type): truefalse
(Category): 6
(Random answers): 0
(Question): Is Support Vector Machine a linear classifier?
(A): True
(B): False
(Correct): A
(Points): 1


(Type): multiplechoice
(Category): 6
(Random answers): 1
(Question): Logistic regression assumes
(A): p(x|y) follows multinomial gaussian distribution
(B): p(x|y) follows poisson distribution
(C): p(x|y) follows bernoulli distribution
(D): No assumption about distribution of p(x|y) 
(Correct): D
(Points): 1



// redo RNN
(Type): multiplechoice  - Multiple answers
(Category): 6
(Grade style): 0
(Random answers): 1
(Question): Back Propagation through time is associated with which of the following algorithm(s)?
(A): Recurrent neural networks
(B): 3 layered fully connected neural network
(C): LSTM
(D): Bi-LSTM
(Correct): A,C,D
(Points): 1


(Type): truefalse
(Category): 6
(Random answers): 0
(Question): Linear regression using gradient descent with sum of squared errors guarantees global minima because of convex function.
(A): True
(B): False
(Correct): B
(Points): 1


(Type): multiplechoice multiple answers
(Category): 6
(Grade style): 0
(Random answers): 0
(Question): Which of the following statement/s is/are true
(A): Data normalization will change the correlation with y variable
(B): Data normalization will not change the correlation with y variable
(C): Data transformation will change the correlation with y variable
(D): Data transformation will not change the correlation with y variable
(Correct): B,C
(Points): 1


(Type): multiplechoice multiple answers
(Category): 6
(Grade style): 0
(Random answers): 1
(Question): Mark all parameters of random forest algorithm
(A): Learning Rate
(B): Maximum depth of any tree
(C): Number of trees
(D): Type of kernel function
(Correct): B,C
(Points): 1


(Type): multiplechoice - multiple Answers
(Category): 6
(Grade style): 0
(Random answers): 1
(Question): Which of the following will help in regularization
(A): Reduce the height of the tree
(B): Increase the height of the tree
(C): Feature bagging
(D): Sample bagging
(Correct): A,C,D
(Points): 1

// redo
(Type): multiplechoice
(Category): 6
(Random answers): 1
(Question): For n-classes (n>2), how many minimum different ROC curves is needed to get AUC score
(A): n
(B): n+1
(C): n-1
(D): 1
(Correct): A
(Points): 1


(Type): multiplechoice
(Category): 6
(Random answers): 0
(Question): A categorical variable has 4 different categories. We can create dummy variables to capture the information of these categories. How many new binary features are sufficient to capture full information.
(A): 1
(B): 2
(C): 3
(D): 4
(Correct): C
(Points): 1


/ redo
(Type): multiplechoice
(Category): 6
(Random answers): 1

11. (Question): What is the time complexity for least square regression with m training examples and p features
>(A): O(pm[sup]2[/sup]) -- pm2

>(B): O(pm) pm

>(C): O(p[sup]3[/sup]m[sup]2[/sup])-- p3m2

>(D): O(p[sup]2[/sup]m) --- p2m
(Correct): D
(Points): 1


(Type): multiplechoice
(Category): 6
(Random answers): 1
(Question): Two Blocks are chosen randomly on a chessboard. What is the probability of having one side in common?
(A): 1/18
(B): 1/64
(C): 1/36
(D): 1/9
(Correct): A
(Points): 1


(Type): multiplechoice
(Category): 6
(Random answers): 1
(Question): Two boxes containing candies are placed on a table. The boxes are labelled B1 and B2. Box B1 contains 7 cinnamon candies and 4 ginger candies. Box B2 contains 3 cinnamon candies and 10 pepper candies. The boxes are arranged so that the probability of selecting box B1 is 1⁄3 and the probability of selecting box B2 is 2⁄3. Suresh is blindfolded and asked to select a candy. He will win a colour TV if he selects a cinnamon candy. If he wins a colour TV, what is the probability that the marble was from the first box?
(A): 6/13
(B): 7/13
(C): 7/33
(D): 6/33
(Correct): B
(Points): 1

//redo
(Type): multiplechoice
(Category): 6
(Random answers): 1
(Question): A 5-input neuron has weights 1, 2, 3, 4 and 4. The transfer function is linear with the constant of proportionality being equal to 1/2. The inputs are 4, 10, 5,20 and 25 respectively. The output will be:
(A): 438
(B): 219
(C): 221
(D): 109.5
(Correct): D
(Points): 1


// redo
(Type): multiplechoice
(Category): 6
(Random answers): 1
(Question): Which of the following statement is correct for t-distributed stochastic neighbor embedding (t-SNE) and principal component analysis (PCA)?
(A): t-SNE is linear whereas PCA is non-linear
(B):  t-SNE and PCA both are linear
(C):  t-SNE and PCA both are nonlinear
(D): t-SNE is nonlinear whereas PCA is linear
(Correct): D
(Points): 1

// to do
(Type): multiplechoice - multiple answers
(Category): 6
(Grade style): 0
(Random answers): 1
(Question): Which of the following statements are true?
(A): Linear SVM is parametric algorithm.
(B): RBF kernel SVM is non parametric algorithm
(C): RBF kernel SVM is parametric algorithm
(D): Linear SVM is non parametric algorithm.
(Correct): A,B
(Points): 1


//redo

(Type): multiplechoice
(Category): 6
(Random answers): 0
(Question): Which of the following metrics, do we have for finding dissimilarity between two clusters in hierarchical clustering?
(A): Single-link
(B): Complete-link
(C): Average-link
(D): All of the above
(Correct): D
(Points): 1

// redo
(Type): multiplechoice multiple answers
(Category): 6
(Grade style): 0
(Random answers): 0
(Question): For a binary classification problem with 3 models each with 80% accuracy. Choose the correct option for majority voting method. Assume train and test are fixed.
(A): No scenario can exist when the accuracy will reach 100%
(B): Some scenario(s) can exist when the accuracy will reach 100%
(C): Minimum accuracy can go below 80%
(D): Minimum accuracy will never go below 80%
(Correct): B, D
(Points): 1


(Type): multiplechoice
(Category): 6
(Random answers): 1
(Question): Which one of the following layers constrains the network to have fixed input image size in object detection deep learning architectures?
(A): Convolution
(B): Max-pool
(C): Fully connected
(D): Activation
(Correct): C
(Points): 1

//redo
(Type): multiplechoice - multiple answers
(Category): 6
(Grade style): 0
(Random answers): 1
(Question): Choose the correct statements from the following
(A): Hard margin SVM can work only when data is completely linearly separable without any errors (noise or outliers)
(B): The  allowance of softness in margins (i.e. a low cost setting) allows for errors to be made while fitting the model (support vectors) to the training/discovery data set.
(C): Slack variables are present in hard margin SVM.
(D): Logistic regression is non linear classifier 
(Correct): A,B
(Points): 1


(Type): multiplechoice
(Category): 6
(Random answers): 1
(Question): Let n x n x 3 be the dimension of input image, where the 3rd dimension represents RGB channels. Then what would be the value of 'n_c' for the convolution filter of size 3 x 3 x n_c ?
(A): n
(B): 1
(C): 3
(D): none of the above
(Correct): C
(Points): 1


(Type): multiplechoice multiple answers
(Category): 6
(Grade style): 0
(Random answers): 0
(Question): Which of the following distance measure is/are not sensitive to feature length?
(A): Eucliedean distance
(B): Cosine distance
(C): Manhattan distance
(D): Jaccard distance
(Correct): B,D
(Points): 1


(Type): multiplechoice
(Category): 6
(Random answers): 0
(Question): Which of the following statement is valid
(A): Maximum aposteriori uses prior probability
(B): Maximum Likelihood Estimation uses prior probability
(C): Both Maximum aposteriori and Maximum Likelihood Estimation uses prior probability
(D): None of Maximum aposteriori and Maximum Likelihood Estimation uses prior probability
(Correct): A
(Points): 1


// redo
(Type): multiplechoice
(Category): 6
(Random answers): 0
(Question): Which of the following measure is related to "population coverage by a rule" in association analysis?
(A): Confidence
(B): Lift
(C): Support
(D): None
(Correct): C
(Points): 1




(Type): multiplechoice multiple answers
(Category): 6
(Grade style): 2
(Random answers): 0
(Question): Popular machine learning term 'Bag of words' model can be used in which of the following tasks?
(A): Topic Modeling
(B): Image Classification
(C): Image Recognition
(D): Scene Classification
(E): Language Modeling
(Correct): A,B,C,D,E
(Points): 1