# Basics 
Cover till LSTM and RNN from the Andrew notes, and read the Alammar blog for word2vec and how to train it.
- [AndrewNG course-5](../DeepLearning.ai-Summary-master/5-%20Sequence%20Models)
- [RNN]() 
- [LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [Word2Vec: How to Train - Jay Alammar](https://jalammar.github.io/illustrated-word2vec/), Read Glove and bagOfwords, skipgram both 

# Transformers
1. [Seq2Seq model with attentions](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
2. [Transformers](https://jalammar.github.io/illustrated-transformer/) 

Transformers with Attention is all you need understanding.


# BERT
1. [Visual Guide to BERT](https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)
2. [Explain Transformers](https://jalammar.github.io/explaining-transformers/)


# Additional Refrences
- [Github Repo For best resources DL-4-NLP](https://github.com/brianspiering/awesome-dl4nlp)
- [PPT and books links from Standford](https://web.stanford.edu/~jurafsky/slp3/)
- [Book by Jay Alammar](https://a.co/d/e7IrkAO)