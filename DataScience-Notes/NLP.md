# Basics 
Cover till LSTM and RNN from the Andrew notes, and read the Alammar blog for word2vec and how to train it.
- [AndrewNG course-5](../DeepLearning.ai-Summary-master/5-%20Sequence%20Models)
- [RNN]() 
- [LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [Word2Vec: How to Train - Jay Alammar](https://jalammar.github.io/illustrated-word2vec/), Read Glove and bagOfwords, skipgram both 


# Data Preparations

- [Tokenizers](https://huggingface.co/docs/transformers/en/tokenizer_summary)
- [Sentence Embedding Models]()

# Transformers
1. [Seq2Seq model with attentions](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
2. [Transformers](https://jalammar.github.io/illustrated-transformer/), [video](https://www.youtube.com/watch?v=ISNdQcPhsts&ab_channel=UmarJamil)

Transformers with Attention is all you need understanding.


# BERT
BERT is encoder only model based on transformers
1. [Visual Guide to BERT](https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)
2. [Explain Transformers](https://jalammar.github.io/explaining-transformers/)
3. [Bert for Sentence Embedding](https://arxiv.org/abs/1908.10084), [My Notes]()


# GPT like models
GPT is decoder only model, used for next token prediction like task. Understand it in extension to the Transformer and as a alternative to the BERT

# Deep-seek like model with agentic flow in model
learn more about them and in general some understanding of advancement Deep-seek has made.
[DeepSeek Technical Report](https://arxiv.org/pdf/2412.19437v1)

Most of the benchmark discussed in NLP paper define the common problem and objectively report the performance.
For example the code-eval report model performance based on test case passed by the generated code.
Note is these benchmark are way different from loss or cost function on which model is trained.


# Additional Refrences
- [Github Repo For best resources DL-4-NLP](https://github.com/brianspiering/awesome-dl4nlp)
- [PPT and books links from Standford](https://web.stanford.edu/~jurafsky/slp3/)
- [Book by Jay Alammar](https://a.co/d/e7IrkAO)


# Question Asked Related to NLP

1. How do you fine tune a GPT like model, What are its inputs and what are the matrices and cost function to evaluate its performance. 