## First Notes

Understand what is Decision Tree and Random Forest rest is just very simple.

[Read from here](https://www.analyticsvidhya.com/blog/2016/04/tree-based-algorithms-complete-tutorial-scratch-in-python/)

Succession is as follows
1. Decision Tree
2. Random Forest
2. AdaBoost
2. Gradient Boosting
3. XGBoost

# Other Usage to Understand (Refer Later in this guide)
1. Feature Extraction using GBDT
2. Feature Reduction using GBDT (Feature Importance/ selection)
3. Data Imputation using GBDT (missing value imputation)

The Basic idea Behind Random Forest is the feature which is at the top of the tree is the most important feature. The more the feature is at the top of the tree the more important it is. Moreover it calculates the Information gain by using some particular formula. The more the information gain the more important the feature is.


# Decision Tree Matrices

Sure! Here's your content rewritten in **Markdown** format with improved structure, readability, and clear visual explanations to help build deep intuition around **Decision Tree splitting criteria**.

---

# üå≥ Decision Tree Splitting Criteria ‚Äì Core Concepts

The algorithm selection for decision trees largely depends on the **type of target variable** (categorical vs. continuous). Below are the **four most commonly used algorithms** to determine the best feature split:

---

## üîπ 1. **Gini Index**

> _"Probability of randomly chosen elements being of the same class."_

- **Used for**: Classification (categorical target)
- **Splits**: Binary only (used in **CART** ‚Äì Classification and Regression Trees)
- **Interpretation**: Lower Gini ‚Üí better purity (homogeneity)

### üîß How to Calculate Gini:
1. For each sub-node:  
   ```math  Gini = p^2 + q^2 ```math   
   where `p` = probability of success, `q` = probability of failure.
2. For the split:  
   ```math  Gini_{split} = \sum_{i} \left(\frac{n_i}{N} \cdot Gini_i\right) ```  

---

### üìä Example: Splitting on "Gender" and "Class"

| Split | Node | Success | Failure | Gini Node | Weight | Weighted Gini |
|-------|------|---------|---------|-----------|--------|----------------|
| Gender | Female | 2 | 8 | 0.68 | 10/30 | 0.227 |
|        | Male   | 13 | 7 | 0.55 | 20/30 | 0.366 |
|        |        |     |   |       |        | **0.59** |
| Class | IX     | 6 | 8 | 0.51 | 14/30 | 0.238 |
|       | X      | 9 | 7 | 0.51 | 16/30 | 0.272 |
|       |        |     |   |       |        | **0.51** |

‚úÖ **Gender** split is better since **0.59 > 0.51**

> **Gini Impurity** = 1 ‚àí Gini

---

## üîπ 2. **Chi-Square**

> _"Statistical significance between observed and expected frequencies."_

- **Used for**: Classification (categorical target)
- **Splits**: Binary or multi-way (used in **CHAID**)
- **Interpretation**: Higher Chi-square ‚Üí more significant split

### üîß How to Calculate Chi-Square:
1. For each class:  
   ```math  \chi^2 = \frac{(Actual - Expected)^2}{Expected} ```  
2. Sum the Chi-square scores of all classes and nodes.

---

### üìä Example (Split on Gender):

| Node | Actual (Play, Not Play) | Expected (Play, Not Play) | Deviation | Chi-Square |
|------|--------------------------|----------------------------|-----------|------------|
| Female | 2, 8 | 5, 5 | -3, +3 | 3.6 |
| Male | 13, 7 | 10, 10 | +3, -3 | 1.8 |
| **Total** | | | | **5.4** |

‚úÖ Gender again is more significant than Class for splitting.

---

## üîπ 3. **Information Gain (Entropy Based)**

> _"How much information is reduced after the split."_  
> Used by: **ID3**, **C4.5**

- **Used for**: Classification (categorical target)
- **Entropy** measures disorder:  
  ```math  Entropy = -p \log_2(p) - q \log_2(q) ```  
- **Information Gain** = Entropy(Parent) ‚àí Weighted Entropy(Children)

---

### üìä Example: Entropy Calculations

| Node      | Success | Failure | Entropy |
|-----------|---------|---------|---------|
| Parent    | 15 | 15 | 1.0     |
| Female    | 2 | 8  | 0.72    |
| Male      | 13 | 7 | 0.93    |

> Weighted Entropy (Gender split) =  
> ```math  \frac{10}{30} \cdot 0.72 + \frac{20}{30} \cdot 0.93 = 0.86 ```math   
>  
> Information Gain = 1.0 ‚àí 0.86 = **0.14**

| Node      | IX | X | Entropy |
|-----------|----|---|---------|
| Class IX  | 6 | 8 | 0.99     |
| Class X   | 9 | 7 | 0.99     |
> Weighted Entropy = **0.99** ‚Üí IG = 1.0 ‚àí 0.99 = **0.01**

‚úÖ **Split on Gender** gives higher **information gain**

---

## üîπ 4. **Reduction in Variance (for Regression)**

> _"Split that reduces output variance the most."_  
> Used in: **Regression Trees**

- **Used for**: Continuous target variable
- **Variance** =  
  ```math  \frac{1}{n} \sum (x_i - \bar{x})^2 ```  

---

### üìä Example:

Assign `1` for "Play", `0` for "Not Play".

- Parent Mean = 0.5  
  ```math  \text{Var}_{root} = 0.25 ```  

| Node   | Mean | Variance |
|--------|------|----------|
| Female | 0.2  | 0.16     |
| Male   | 0.65 | 0.23     |

> Weighted Variance (Gender) =  
> ```math  \frac{10}{30} \cdot 0.16 + \frac{20}{30} \cdot 0.23 = 0.21 ```  

| Node   | Mean | Variance |
|--------|------|----------|
| Class IX | 0.43 | 0.24 |
| Class X  | 0.56 | 0.25 |

> Weighted Variance (Class) = **0.25**

‚úÖ Gender gives lower variance ‚Üí better split

---

# üéØ Summary

| Criterion | Type of Target | Algorithm | Splitting Style | Best When |
|-----------|----------------|-----------|------------------|-----------|
| **Gini** | Categorical | CART | Binary | Fast + Pure |
| **Chi-Square** | Categorical | CHAID | Multi-way | Stat significance |
| **Info Gain (Entropy)** | Categorical | ID3, C4.5 | Binary | Info-theoretic split |
| **Variance Reduction** | Continuous | CART | Binary | Regression tasks |

---

> ‚úÖ Choose the splitting criterion **based on the target type and interpretability**.  
> For classification: Gini and Info Gain dominate.  
> For regression: Variance reduction is ideal.

---


## AdaBoost 

[StatQuest Video](https://www.youtube.com/watch?v=LsK-xG1cLYA&ab_channel=StatQuestwithJoshStarmer)

AdaBoost is a **Boosting** algorithm that combines multiple weak classifiers to create a strong classifier. It focuses on correcting the errors made by previous classifiers, making it effective for various classification tasks.
- **Weak Learner**: A model that performs slightly better than random guessing.
Stump : A decision tree with only one split (1 node + 2 leaves).
- In this the next stump will be seeing data that is oversampled from the example where first stump made a mistake.
- This is done using the formula given below. But that is it. increase number of sample of the misclassified data and decrease the %number of sample of the correctly classified data.

![img_3.png](decision_trees/img_3.png)

Three main ideas behind AdaBoost:
1. **Sequential Learning**: Each weak learner is trained on the errors of the previous one.
2. **Weighted Voting**: Each weak learner's vote is weighted based on its accuracy.
3. **Focus on Hard Examples**: More weight is given to misclassified samples, making the model focus on harder-to-classify instances.
4. **Final Prediction**: The final prediction is made by combining the weighted votes of all weak learners.
![img_4.png](decision_trees/img_4.png)

Mathematics and other intution and as given below
---

## **üìå Introduction**
- AdaBoost is mostly used with **decision stumps (1 node + 2 leaves)**.
- Compared with **Random Forests** to highlight key differences.

---

## **üå≥ Random Forest vs. AdaBoost**

| Concept                        | Random Forest                       | AdaBoost                              |
|-------------------------------|--------------------------------------|----------------------------------------|
| Learner Type                  | Full Decision Trees                 | Weak Learners (Stumps)                |
| Voting Power                  | Equal for all trees                 | Weighted voting based on accuracy     |
| Independence of Trees         | Trees are built independently       | Trees are built sequentially          |
| Learning Influence            | No influence from other trees       | Each stump is influenced by prior errors |

---

## **üìò Key Concepts of AdaBoost**

1. Combines many **weak learners** (usually stumps).
2. Some stumps get **more say** based on performance.
3. Each stump is **built sequentially**, correcting the errors of the previous ones.

---

## **üß± Building AdaBoost ‚Äì Step-by-Step**

### **1. Initialize Sample Weights**

- Start with **equal weight** for each sample:
  ```math
  w_i = \frac{1}{N}
  ```

### **2. Train First Stump**
- Try splitting on each feature.
- Use **Gini Index** to find the best split.
- Choose feature with lowest Gini (best separation).

---

### üìä **Example Gini Calculation Table**

| Feature        | Gini Index | Error Count | Error Rate |
|----------------|------------|-------------|------------|
| Chest Pain     | X1         | 3           | 3/8        |
| Blocked Artery | X2         | ?           | ?          |
| Weight         | **Lowest** | 1           | 1/8        |

‚úÖ **Choose "Weight" for the first stump**

---

### **3. Calculate Amount of Say**
Formula:
```math
\text{Say} = \frac{1}{2} \cdot \log\left(\frac{1 - \text{Error}}{\text{Error}}\right)
```

üìà **Intuition:**

- Low error ‚Üí High positive say ‚Üí Trusted vote
- Error = 0.5 ‚Üí Say = 0 (random guess)
- High error ‚Üí Negative say ‚Üí Vote flips

---

### **Example: Say Calculation**

For `error = 1/8`:

```math
\text{Say} = \frac{1}{2} \cdot \log\left(\frac{7/8}{1/8}\right) ‚âà 0.97
```

---

## **‚ôªÔ∏è Updating Sample Weights**

### **Incorrectly Classified:**
```math
w_i \leftarrow w_i \cdot e^{\text{Say}}
```

### **Correctly Classified:**
```math
w_i \leftarrow w_i \cdot e^{-\text{Say}}
```
$w_i \leftarrow w_i \cdot e^{-\text{Say}}$

üßÆ Normalize all weights to sum to 1.

---

## **üì¶ Two Options for Next Iteration**

1. **Weighted Gini** for next stump.
2. **Resample Dataset** based on weights (bootstrap sampling).

üìå Samples misclassified earlier get **duplicated more** ‚Üí More influence.

---

### üé≤ Sampling Example:

If sample A has weight = 0.33  
‚Üí It might appear **4 times** in new data of same size.

---

## **üó≥Ô∏è Final Classification ‚Äì How AdaBoost Predicts**

1. Each stump votes based on its decision.
2. Votes are **weighted by Say** value.
3. Total sum of "say" values for each class.
4. Class with higher sum wins.

üß† **Think of it like:**
```text
HAS_DISEASE: stump1 (0.97) + stump3 (0.42) = 1.39  
NO_DISEASE : stump2 (0.33) + stump4 (0.29) = 0.62  
‚Üí Predict: HAS_DISEASE
```


---

## Gradient Boosting
[StatQuest Video - only one is enough](https://www.youtube.com/watch?v=3CC4N4z3GJc&list=PLblh5JKOoLUJjeXUvUE0maghNuY2_5fY6&index=1&ab_channel=StatQuestwithJoshStarmer)

Sure! Here's a cleaner, more readable version of your explanation using Markdown, with enhanced language and structure while preserving the image references:

---

## üå≤ Step-by-Step Intuition Behind Gradient Boosting

We‚Äôll walk through the **Gradient Boosting** process using a simple example to build deep intuition. Visuals will help explain each concept clearly.

---

### üîπ Initial Step: Start with a Baseline Prediction

We begin by predicting the **average of the output values**. This serves as the initial model.

> üìå This is the prediction from the **zeroth** tree.  
> üîÅ Next, we compute the **residuals** (i.e., the difference between actual values and the predicted average).

![img_5.png](decision_trees/img_5.png)

---

### üîπ Tree Structure Differences: AdaBoost vs Gradient Boosting

In **AdaBoost**, each tree is a decision stump (i.e., has only 2 leaf nodes).  
But in **Gradient Boosting**, trees can have **more than 2 leaves**, though the number of leaves is typically fixed.

![img_6.png](decision_trees/img_6.png)

---

### üßÆ Step 0: Compute Residuals

Using the baseline prediction, we calculate the residuals for each data point. These residuals represent what‚Äôs left for the next tree to learn.

![img_7.png](decision_trees/img_7.png)

---

### üå≥ Step 1: Fit a Tree to the Residuals

Now, we fit a new decision tree to these residuals. This tree tries to predict the errors made by the previous model.

![img_8.png](decision_trees/img_8.png)

---

### üçÇ Step 2: Restrict Leaf Nodes and Average Predictions

We restrict the number of leaf nodes to `m` (e.g., 4), where `m < M`.  
Since multiple data points may fall into a single leaf, we **average their residuals** in each leaf to get the leaf output.

![img_9.png](decision_trees/img_9.png)  
![img_10.png](decision_trees/img_10.png)

---

### ‚ûï Step 3: Update Predictions

We update our original predictions by adding the output of this residual tree.  
However, to avoid overfitting, we **scale this update** by a factor called the **learning rate** (typically 0.1).

```math
\text{New Prediction} = \text{Old Prediction} + 0.1 \times \text{Residual Tree Output}
```

![img_11.png](decision_trees/img_11.png)  
![img_12.png](decision_trees/img_12.png)

---

### üîÅ Repeat the Process

We continue this process iteratively:
- Compute residuals
- Fit a tree to the residuals
- Update the predictions with a scaled version of the new tree‚Äôs output

Repeat for **N trees** to build a strong ensemble!

![img_14.png](decision_trees/img_14.png)  
![img_15.png](decision_trees/img_15.png)

---

> ‚úÖ That‚Äôs Gradient Boosting in action ‚Äî an additive model built sequentially to minimize the residual error at each step.

## XGBoost 

Just more advance version of GBDT in that it apply regularization and tree pruning and other things.
[StatQuest](https://www.youtube.com/watch?v=OtD8wVaFm6E&list=PLblh5JKOoLULU0irPgs1SnKO6wqVjKUsQ&ab_channel=StatQuestwithJoshStarmer)


## GBDT for Feature Extraction and Feature reduction

Great follow-up! Let‚Äôs break down how **decision tree-based algorithms** are used for **feature reduction** and **feature extraction**, two slightly different concepts:

---

## üåü 1. **Feature Reduction (Feature Selection)**

This means **removing irrelevant or redundant features** from your dataset. Tree-based models help with this in the following ways:

### ‚úÖ **How it works**:
- Train a **decision tree model** (or a forest/boosted model).
- Get the **feature importances** (e.g., `.feature_importances_` in scikit-learn).
- **Set a threshold** (e.g., keep only top-k features or those above a certain importance score).
- Drop the less important features from your dataset.

### üß† Why it works:
- Tree-based models naturally focus on features that **split the data well**, so features that aren‚Äôt used much (or at all) are likely not helpful.
- It reduces **dimensionality**, speeds up model training, and can help avoid overfitting.

### üìå Example:
```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel

rf = RandomForestClassifier()
rf.fit(X_train, y_train)

# Automatically reduce features based on importance threshold
selector = SelectFromModel(rf, threshold="mean", prefit=True)
X_train_reduced = selector.transform(X_train)
```

---

## üß¨ 2. **Feature Extraction (Constructing New Features)**

This means **creating new features** based on the structure of the model itself, especially with decision trees and tree ensembles.

### üå≥ Tree-Based Feature Extraction Techniques:

#### A. **Tree Path Features** (Embedding from Leaf Indices)
- You run your data through the trained trees and **record the leaf node index** for each sample in each tree.
- These leaf indices can be **one-hot encoded** or embedded into vectors.
- This creates a new, dense representation of the original features.

#### üí° Used in:
- **Gradient Boosted Decision Tree + Logistic Regression**
- **XGBoost + Linear Models**
- Kaggle competitions & production pipelines

```python
import xgboost as xgb

model = xgb.XGBClassifier()
model.fit(X_train, y_train)

# Extract leaf indices (each sample‚Äôs path in each tree)
leaf_indices = model.apply(X_train)

# One-hot encode or use directly as features for downstream model
```

#### B. **Stacking with Tree Outputs**:
- Train a tree-based model to **learn non-linear patterns**, then feed its output (probabilities or leaf-based features) into another model like logistic regression or neural networks.

---

## üîÅ Feature Reduction vs Feature Extraction ‚Äì Quick Recap:

| Aspect                | Feature Reduction                          | Feature Extraction                          |
|-----------------------|---------------------------------------------|---------------------------------------------|
| Goal                 | Remove irrelevant features                  | Create new informative features             |
| Output               | Subset of original features                 | New feature representation                  |
| Tree Role            | Provide feature importance                  | Provide structural info (e.g., leaf nodes)  |
| Typical Tool         | Random Forest, XGBoost, LightGBM            | XGBoost, LightGBM with `.apply()`           |

---

Want a full code demo using XGBoost or scikit-learn for both reduction and extraction?



### GPT version summary


Here's your complete intuition-first **revision notes** for all major **tree-based algorithms**:  
‚úÖ **Decision Tree**, üå≤ **Random Forest**, ‚ö° **AdaBoost**, üåÑ **Gradient Boosting**, and üöÄ **XGBoost**  
‚Äî with examples, diagrams, and tables to build a deep understanding.

---

# üå≥ 1. Decision Tree

### ‚úÖ Basic Idea:
A decision tree **splits data** into subsets based on feature values to create **pure leaves** (i.e., each node has mostly one class or similar values).

### üß† Key Concepts:
- Uses **recursive binary splits**.
- Selects the "best" feature at each node using metrics like:
  - **Gini Impurity** (used in CART)
  - **Entropy / Information Gain** (used in ID3/C4.5)
- **Greedy algorithm**: makes the best local decision at each split.

### üå± Example:

Imagine classifying fruits based on size and color:

```
                [Is Color = Green?]
                /               \
            Yes                  No
         [Size > 3?]         [Class: Apple]
         /        \
    Yes           No
[Class: Mango]   [Class: Grapes]
```

### üìä Comparison Table:

| Pros                          | Cons                              |
|-------------------------------|------------------------------------|
| Easy to interpret             | Overfits easily                   |
| No feature scaling needed     | Unstable to small data changes    |
| Works with both types         | Not very accurate alone           |

---

# üå≤ 2. Random Forest

### ‚úÖ Basic Idea:
An ensemble of **many decision trees** trained on **random subsets** of data and features.

- Each tree votes and the **majority class (classification)** or **average prediction (regression)** is used.

### üß† How It Improves Decision Trees:
| Feature      | Why It Helps             |
|--------------|--------------------------|
| Bagging      | Reduces variance         |
| Random Features | Adds decorrelation between trees |
| Ensemble     | Smoothens predictions    |

### üå≤ Visualization:

Imagine building 3 trees like this from bootstrapped data:
```
Tree 1 ‚Üí üçé  
Tree 2 ‚Üí üçå  
Tree 3 ‚Üí üçé  
Final Vote = üçé
```

### üìä Comparison Table:

| Pros                          | Cons                            |
|-------------------------------|----------------------------------|
| High accuracy                 | Slower to predict                |
| Handles missing data          | Not easily interpretable         |
| Feature importance available  | Large models = more memory       |

---

# ‚ö° 3. AdaBoost (Adaptive Boosting)

### ‚úÖ Basic Idea:
- Train **weak learners** (usually shallow trees).
- At each step, **increase weights** on misclassified samples.
- Focus future learners on **‚Äúhard‚Äù samples**.

### üß† Intuition:
Each learner corrects the previous one's mistakes.

### üéØ Example:
```
1st tree: Misclassifies A, C  
‚Üí Boost A & C's weight

2nd tree: Focuses more on A, C  
‚Üí Misclassifies B  

3rd tree: Focuses more on B  
‚Üí Final prediction: weighted vote
```

### ‚öñÔ∏è Final prediction is a **weighted sum** of all learners.

### üìä Comparison Table:

| Pros                      | Cons                         |
|---------------------------|-------------------------------|
| Works well on imbalanced data | Sensitive to noisy data      |
| Boosts weak learners       | Sequential ‚Üí can't parallelize |
| Often better than random forests on tabular data | May overfit if too many rounds |

---

# üåÑ 4. Gradient Boosting

### ‚úÖ Basic Idea:
Like AdaBoost, but instead of adjusting weights, each learner fits to the **residual errors** (i.e., the difference between actual and predicted values).

### üß† Steps:
1. Predict mean value (first model).
2. Compute residuals:  
   \[
   \text{residual} = y - \hat{y}
   \]
3. Train next tree to **predict residuals**.
4. Add that tree‚Äôs output to previous predictions.

### üìâ Example:
| Round | Model Output | Residual      |
|-------|---------------|---------------|
| 0     | 10 (mean)     | Actual - 10   |
| 1     | +2            | Actual - 12   |
| 2     | +1.5          | Actual - 13.5 |

Final prediction:  
\[
\hat{y} = 10 + 2 + 1.5 = 13.5
\]

### üìä Comparison Table:

| Pros                         | Cons                          |
|------------------------------|-------------------------------|
| Very accurate                | Slower than random forest     |
| Handles non-linear patterns  | Needs careful tuning (learning rate, depth) |
| Can be regularized           | Can overfit on small data     |

---

# üöÄ 5. XGBoost (Extreme Gradient Boosting)

### ‚úÖ Basic Idea:
An **optimized version of Gradient Boosting** with:
- **Regularization (L1 & L2)**
- **Tree pruning**
- **Parallelization**
- **Efficient memory usage**

### üöß Improvements Over Vanilla GBM:
| Feature             | Why it matters                          |
|---------------------|------------------------------------------|
| Regularization      | Reduces overfitting                     |
| Pruning (best-first)| Faster & better trees                   |
| Histogram-based split | Faster with large data                |
| Sparse-aware        | Handles missing values well             |
| GPU support         | Scalable for big datasets               |

### üîç Diagram (Simplified):
```
Boosting Round 1 ‚Üí Learns residuals
Boosting Round 2 ‚Üí Learns new residuals
‚Ä¶
Final Prediction = Sum of Trees
```

---

### üìä Final Comparison Summary:

| Model        | Key Idea                              | Accuracy | Speed | Overfit Risk | Interpretability |
|--------------|----------------------------------------|----------|-------|---------------|------------------|
| Decision Tree | Split data recursively                 | Medium   | High  | High          | High             |
| Random Forest| Bagging + random feature selection     | High     | Medium| Low           | Medium           |
| AdaBoost     | Boosting by reweighting errors         | High     | Slow  | Medium        | Low              |
| GBM          | Boosting by fitting residuals          | Very High| Slow  | High          | Low              |
| XGBoost      | Regularized, optimized GBM             | Very High| Fast  | Low/Medium    | Low              |

---

Would you like a cheat sheet version of this with visuals and color-coded highlights for your revision PDF?
