
GPU utilization imporvement
We used work in edgeAI infra environment where multiple camera stream their input to our computation device. This device has a GPU attached to it. Every single solution (kind of a black box) take image stream as input and output key APIs. Now the number of solution that can be run on single device depends on GPU capacity it used to run only 4 solution.
Now to run more number of solution more machine has to be added, because we were growing rapidly in number of sites and adding more and more camera each day we were limited by the device avaiable. Moreover there was additional cost attached to each device being procurred. Procurement and clearance takes a long time. Moreove there was GPU shortage at that time so it effects the pace at which we deploy solution. overall it was an overarching problem and need to use the computation efficiently.

Everyone in the team was aware of the issue and working on multiple aspect like reducing model sizes, performing quanitization, using lower fps but all had their compromises. At that time My manager gave me the task to estimate the future computation requirement based on expansion proposed. I was running the GPU profiling and noticed that GPU RAM was 100% occupied but the GPU util was hovering around 40-50% with sporadic peaks. This was happening because individual solution was running the same model but because of black box nature of solution each solution work in solace and load the same model again and again in GPU. This resulted in bottleneck being the GPU RAM and not the utility. How did we measure is standalone model had 20fps peak rate while multiple solution were peaking at 10fps so there was large scope of improvement but the architecture of the solution limited it.


There were limited option to deal with this. Because solution are one topic input constrain as limited by the design of edgeml enviornment in which we run the solution. I went on to discuss this problem and potential solution for this from Infra engineering team, I had a overall aim and solution in mind but I was not sure how could that be implemented. upon discussion with one of the engineering lead I came to know the python sdk of the platform had the option to connect to multiple topic from config file. Based on this input I architecting a solution which works as follows.

There will be a central solution lets call it the engine of the machine, this and only this will be responsible for running the ML model. I implemented a queuing mechanisim that allows to run batch infer on the images and the respective solutions will subscribe to the inference output topic of this centralized solution and perform KPIs calculation and other task like email and sms based on their configurations.

One additional benefit we got was because now we were inferring images in batches the GPU net through put can peaked till 24 fps, a remarkable 20% over the single best benchmark. Moreover this also removes the number of solution limitation because now we can run any number of solution at lower through put. thing to note was previosuly it was not possible because each solution would require GPU RAM which was a limited comoddity. What happen later was we were able to load 10 solution with 10 camera feed into single machine because other CPU operation becomes bottleneck after this. Overall it was huge performance gain and moreover it promoted engineering team to take architecture level changes to allow this kind of configuration for future deployement from the product level so that the messaging queue and other batching process could be optimize at docker level.

