OB-Ask Me Anything ChatBot: Natural language Query & insight generation using LLMs for technical dashboards.
- Developed novel data insight generation tool using Langchain's MRKL agent with Re-Act framework & Zero & Few-shot
Chain of Thoughts prompting techniques.
- Implemented Auto-CoT for process self-improvement & developed prompt-injection detection framework.
- Patented the process & showcased in JCI's Tech-Challenge'24 Finals as one of the top eight, out of 3,000+ submissions.


THis project started with the theme to create a company wide ask me anything chatbot. It was proposed by my collegous chenlu and wenwen, it was selected by my boss rajiv. We started working on the idea with 5 main goals and stages. first ask general information query, ask a reasoning question, utilize data to generate inference like an SME and lastly based on the insight suggest action user can take.

At the initial stages everything was ambigious so I went on to look for tools and technology that people uses to develop such kind of agentic AI application. I purused a course by AndrewNG and Langchain founder on developing application. I discovered the latest advancement in the field like various models (like llama, chatgpt and others),
learned about different style of agentic AI system like zero-shot and few shot learning, developing MRKL agent.
I purused a course on Developing RAG application, in which one of the notable advancement we noticed was using sentence window retreival with overlapping size and how to about maximal marginal retrieval. I prepared all the background required to create agentic AI application. We were lucky to communicate with other teams who were also working on devloping agentic AI application, we collaborated with them to know about their learning and what is working for them. where we discvered about the sentence window retrieval. In this technique sentence of different sizes are stored in vector database so that vector matching a fixed window size is used for vector search but the whole paragraph around that text is used for context. and senetence of variable sizes were used. later This idea help us a lot in improving the accuracy of our context.

I searched for different tech stack people uses for storing vector databases and generating embedding, and we were in azure envionmnet so we used GPT-embedding api and started with chromaDB which was hosted locally for the POC stage.

We develop the first phase of the project in which we showcased feasibilty of first two goals out of original. Because the project was tagged as an R&D effort we needed to showcase the real growth and potential in the project. We demoed it to different teams and gathered usggestion on improvement.

The first architecture involve a chain of thought based model  where we used langchain and MRKL agent to support this operation in this we have to parse lots of text from the output to code or different agents that langchain provides at that time. But what was happening is sometime the parsing may fail and the whole operation may fail to conclude or take a large amount of time to produce results.

At that time I proposed to develep a custom agents from scratch and removing langchain from action part of the project. This also involved an overall framework that generate output in xml format and improving the parsing accuracy of the system. We used langchain wherever it provides boiler plate code and inherited fucntion in which we require more accuracy. The overall code was a generic framework which could utilize this xml based schema based on minimal requirement prompt from user.

The project got good traction in initial stages and was showcased to CTO of JCI and got approval for further development. As the project was growing we felt the need to improve on our vector database, we were using a small instance of vectorDB which had bugs and stability issues. I researched about the other vector databases and found out from other teams PGvector to be a good solution allow SQL style querying and vector search capabilities. This allow us to implement advance sentence retreival techniques that I discussed earlier. THere was a conflict because the architect opinion was to keep using chromaDB and wait for the project to scale before incurring bills for PGvector. But at the current stage I felt a bottlleneck in effciency and overall project performance. This was not understandable from architect point of view because vector database seemed very similiar in terms of features. I discssed this with my manager and prepared a presentation for the whole team discussing about the various tech stack availaible for vector databases and what are the advantages of them. I also discussed the future goals and improvement scope for the project, in which I discussed how important it is to have refined knowledge base in vector DB to improve accuracy of the output and how can we gather this data as we develop the application.

After discussing this and showing the research work, my manager approved the postgres instance for the project. and result was over the time we were able to accrue a large database from the early user . We used this to gather the response of the LLMs for the initial user query and labeled them to be correct or not correct. This example dataset also became few shoot example, like when some one ask a similar question we can query the DB for similiar question and feed them as sample to current question. We observed a significant improvement in our responses quality by LLM.

When the first stage of the project was developed we went on to target the reasoning part of it. JCI had customer support team where SME of system looks into the customer system and diagnose, what is the problem. The knowledge and past user experience data this team posses was very cruical for us to understand to develop intution about the data. being new to the field I learned through the training document of teams and collaborated with multiple team memebers to learn about their thinking process when debugging the system. My motive was to gather all the different knowledge sources to have a comprehensive knowledge based avaialible for us to use. Over the month I sat with multiple people and digesated this knowledge and also included that in the systme database. We also collaborated with Engineering team where we showcased this and they were exited to see this, we got to know about the documention and user guide of various software availaible in very nice format so we included that as well. over all through the project we kept and open mind and discuss the idea with many teams and gathered feedback and knowledge to improve the project.

Developing agentic AI application for building requires understanding of the rdf graph as well, I devloped quick understanding of digital twins system inplace and rdf-graph using pysql whichthe team was using. Initial prottype was to implement RAG on the dump of this graph but this was not handeling the heiarchical information stored in the graph, because user query may be 3-4 level deep in the graph and require to travel it one level at a time. This is a cruical information that SME also uses but they had this info using the digital twin and excel sheets, I noticed this weakness and implemented an efficient solution that used an langchain style chain of thought based Agent specific to query in pysql grpah and provide additional context of the building.

There was one guy Samit, he was the sales representaive of JCI and one of the project of JCI he was selling was about imporving energy efficiency and investing in JCI equipment to save cost. He and his team was expert in the field of diagnosis, we came to know they have devloped a framework which they sell to client in which person look into the system and detect diagnois point using objective measure. By collobartion we were able to implement that framework in our code base which became part of the core module of our project.

ultimately the system was able to diagnisze a given site data based on 7 parameters that we had created and also suggested actions that user can take to improve their energy efficieny and also helping them meet their carbon emission goals.

while we were working on this I was also researching on competiton and see for scope of improvement in our capabilities. Plotly had the feature where user can ask question from the graph and we were providing the feature only at the dashboard level and we got the response from our user that our graphs and dashboard are very technical in nature and they don't understand most of the metrics and representation.
THe initial idea was being devloped but hit bottle neck because of the high cost of LLMs and generic nature of output so I quickly implemeted this feature using our existing code base and showcased to product team and this reached to senior product manager. ultimaltely it was decide to add the graphAI feature as the first phase of the project in future release cycles.


Later on this idea was submitted to the company wide techchallenge and I presented this idea and it was selected as top-6
